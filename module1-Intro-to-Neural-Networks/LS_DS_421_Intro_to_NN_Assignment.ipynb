{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LS_DS_431_Intro_to_NN_Assignment.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dVfaLrjLvxvQ"
      },
      "source": [
        "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
        "<br></br>\n",
        "<br></br>\n",
        "\n",
        "# Neural Networks\n",
        "\n",
        "## *Data Science Unit 4 Sprint 2 Assignment 1*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wxtoY12mwmih"
      },
      "source": [
        "## Define the Following:\n",
        "You can add image, diagrams, whatever you need to ensure that you understand the concepts below.\n",
        "\n",
        "### Input Layer: \n",
        "The layer of a neural network that takes the \"raw\" input data. The first layer of the network. \n",
        "\n",
        "### Hidden Layer:\n",
        "The layers that are not directly reachable (not input or output). Take input from either the input layer or another hidden layer and output to anther hidden layer or the output layer. \n",
        "\n",
        "### Output Layer:\n",
        "The layer of a neural network that produces the prediction (\"consumable\" output). It takes its input from the last hidden layer in a network. \n",
        "\n",
        "### Neuron:\n",
        "A single building block of a NN. Has an associated bias and a weight for every node in the previous layer (doesn't pertain to input layer neurons). It can be thought of as a function where the input is the previous layers node's activation value, the weight associated to each of those nodes, and a bias. The output is `a(1) = activation_function(np.dot(W, a(0)) + bias)`. The activation function is commonly relu or sigmoid. \n",
        "\n",
        "### Weight:\n",
        "A single weight is a value associated with 2 specific nodes. Can be thought of as the connection between two nodes. Along with bias, weight is the \"memory\" of the network that you are trying to perfect in the training process. \n",
        "\n",
        "### Activation Function:\n",
        "The function that puts a \"threshold\" on the computation using a neuron's inputs, weights and bias. Commonly used activation functions are relu, sigmoid, and step. \n",
        "\n",
        "### Node Map:\n",
        "A type of diagram that shows the structure of a NN. \n",
        "\n",
        "### Perceptron:\n",
        "The simplest NN in existance. Consists of a single output neuron and N input neruons (no hidden layers/neurons)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NXuy9WcWzxa4"
      },
      "source": [
        "## Inputs -> Outputs\n",
        "\n",
        "### Explain the flow of information through a neural network from inputs to outputs. Be sure to include: inputs, weights, bias, and activation functions. How does it all flow from beginning to end?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PlSwIJMC0A8F"
      },
      "source": [
        "The flow of information through a NN starts with the input layers. The size of the input layer is equal to the number of features each instance has (for example a NN that predicts house sale prices using 11 features about the house would have an input layer with the 11 neurons/nodes). From the input layer the values of each input node is passed to each node of the following layer. The following layers \"activation\" (resulting value) will be `a(current_layer) = activation_function(dot_product(weights, a(prev_layer)) + bias)` where `weights` is the values that represent the \"strength\" of the connection between that node and the node the weight corresponds to. The ouput layer acts the same way as hidden layers but commonly the activation function is different. Common output layer activation functions are:\n",
        "- regresssion tasks - no actication function (to allow a large range of output values) \n",
        "- binary classification - sigmoid function (either 1 or 0)\n",
        "- multiclass classification - softmax score (for each class their is outputed a probability that it is that class) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6sWR43PTwhSk"
      },
      "source": [
        "## Write your own perceptron code that can correctly classify (99.0% accuracy) a NAND gate. \n",
        "\n",
        "| x1 | x2 | y |\n",
        "|----|----|---|\n",
        "| 0  | 0  | 1 |\n",
        "| 1  | 0  | 1 |\n",
        "| 0  | 1  | 1 |\n",
        "| 1  | 1  | 0 |"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGtS_6BOfXjF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "data = { 'x1': [0,1,0,1],\n",
        "         'x2': [0,0,1,1],\n",
        "         'bias': [1, 1, 1, 1]\n",
        "       }\n",
        "      \n",
        "correct_outputs = ([1],[1],[1],[0])\n",
        "df = pd.DataFrame.from_dict(data).astype('int')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Sgh7VFGwnXGH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "d3107d2d-dcaa-4196-be96-640e3268a670"
      },
      "source": [
        "##### Your Code Here #####\n",
        "import numpy as np\n",
        "weights = np.random.random((3, 1))\n",
        "weights"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.25347418],\n",
              "       [0.81595359],\n",
              "       [0.26247803]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQAZERJOsDRj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid(x):\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "  sx = sigmoid(x)\n",
        "  return sx * (1-sx)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3ItjA6cfXjV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "21a9c042-a5d5-457d-e954-a9cd1e6580ec"
      },
      "source": [
        "weighted_sum = np.dot(df.values, weights)\n",
        "weighted_sum"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.26247803],\n",
              "       [0.51595221],\n",
              "       [1.07843162],\n",
              "       [1.3319058 ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OD-S9X_1x2R8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "4fcbb477-a696-408e-ef78-96ca95723cdd"
      },
      "source": [
        "preds = sigmoid(weighted_sum)\n",
        "preds"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.56524535],\n",
              "       [0.62620077],\n",
              "       [0.74619707],\n",
              "       [0.7911557 ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92J6x8FGs5vx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "c6b7281a-1b50-437d-98bb-ea63ae0096db"
      },
      "source": [
        "error = correct_outputs - preds\n",
        "error"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.43475465],\n",
              "       [ 0.37379923],\n",
              "       [ 0.25380293],\n",
              "       [-0.7911557 ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esUUppdCxtNd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "28517442-4d92-4d78-ebd3-36e0ada626ae"
      },
      "source": [
        "adjustments = error * sigmoid_derivative(weighted_sum)\n",
        "adjustments"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.10683793],\n",
              "       [ 0.08749644],\n",
              "       [ 0.04806698],\n",
              "       [-0.13072136]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFeAR2Qyx_E4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "b8e4e925-82a8-49ae-eb07-0a41a2e769c3"
      },
      "source": [
        "weights += np.dot(df.values.T, adjustments)\n",
        "weights"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.21024927],\n",
              "       [0.73329921],\n",
              "       [0.37415802]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Xf7sdqVs0s4x"
      },
      "source": [
        "## Implement your own Perceptron Class and use it to classify a binary dataset: \n",
        "- [The Pima Indians Diabetes dataset](https://raw.githubusercontent.com/ryanleeallred/datasets/master/diabetes.csv) \n",
        "\n",
        "You may need to search for other's implementations in order to get inspiration for your own. There are *lots* of perceptron implementations on the internet with varying levels of sophistication and complexity. Whatever your approach, make sure you understand **every** line of your implementation and what its purpose is."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUdBnQKxfXjc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "16e60559-d971-4ee3-bfac-db7887bf912b"
      },
      "source": [
        "diabetes = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/diabetes.csv')\n",
        "diabetes.head()"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pregnancies</th>\n",
              "      <th>Glucose</th>\n",
              "      <th>BloodPressure</th>\n",
              "      <th>SkinThickness</th>\n",
              "      <th>Insulin</th>\n",
              "      <th>BMI</th>\n",
              "      <th>DiabetesPedigreeFunction</th>\n",
              "      <th>Age</th>\n",
              "      <th>Outcome</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6</td>\n",
              "      <td>148</td>\n",
              "      <td>72</td>\n",
              "      <td>35</td>\n",
              "      <td>0</td>\n",
              "      <td>33.6</td>\n",
              "      <td>0.627</td>\n",
              "      <td>50</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>85</td>\n",
              "      <td>66</td>\n",
              "      <td>29</td>\n",
              "      <td>0</td>\n",
              "      <td>26.6</td>\n",
              "      <td>0.351</td>\n",
              "      <td>31</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8</td>\n",
              "      <td>183</td>\n",
              "      <td>64</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>23.3</td>\n",
              "      <td>0.672</td>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>89</td>\n",
              "      <td>66</td>\n",
              "      <td>23</td>\n",
              "      <td>94</td>\n",
              "      <td>28.1</td>\n",
              "      <td>0.167</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>137</td>\n",
              "      <td>40</td>\n",
              "      <td>35</td>\n",
              "      <td>168</td>\n",
              "      <td>43.1</td>\n",
              "      <td>2.288</td>\n",
              "      <td>33</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Pregnancies  Glucose  BloodPressure  ...  DiabetesPedigreeFunction  Age  Outcome\n",
              "0            6      148             72  ...                     0.627   50        1\n",
              "1            1       85             66  ...                     0.351   31        0\n",
              "2            8      183             64  ...                     0.672   32        1\n",
              "3            1       89             66  ...                     0.167   21        0\n",
              "4            0      137             40  ...                     2.288   33        1\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9zMipAofXjh",
        "colab_type": "text"
      },
      "source": [
        "Although neural networks can handle non-normalized data, scaling or normalizing your data will improve your neural network's learning speed. Try to apply the sklearn `MinMaxScaler` or `Normalizer` to your diabetes dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bxm8r0EFfXji",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler, Normalizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "feats = list(diabetes)[:-1]\n",
        "train, test = train_test_split(diabetes)\n",
        "norm = Normalizer()\n",
        "X = norm.fit_transform(train[feats])\n",
        "X_test = norm.transform(test[feats])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-W0tiX1F1hh2",
        "colab": {}
      },
      "source": [
        "##### Update this Class #####\n",
        "\n",
        "class Perceptron(object):\n",
        "    \n",
        "    def __init__(self, niter = 10):\n",
        "      self.niter = niter\n",
        "    \n",
        "    def __sigmoid(self, x):\n",
        "      return 1 / (1 + np.exp(-x))\n",
        "    \n",
        "    def __sigmoid_derivative(self, x):\n",
        "      sx = self.__sigmoid(x)\n",
        "      return sx * (1-sx)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "      \"\"\"Fit training data\n",
        "      X : Training vectors, X.shape : [#samples, #features]\n",
        "      y : Target values, y.shape : [#samples]\n",
        "      \"\"\"\n",
        "      # Randomly Initialize Weights\n",
        "      self.weights = np.random.random((X.shape[1], 1))\n",
        "\n",
        "      for i in range(self.niter):\n",
        "          # Weighted sum of inputs / weights\n",
        "          weighted_sum = np.dot(X, self.weights)\n",
        "          # Activate!\n",
        "          activated_output = self.__sigmoid(weighted_sum)\n",
        "          # Cac error\n",
        "          error = y - activated_output\n",
        "          # Update the Weights\n",
        "          adjustments = error * self.__sigmoid_derivative(weighted_sum)\n",
        "          self.weights += np.dot(X.T, adjustments)\n",
        "      \n",
        "      return self\n",
        "\n",
        "    def predict(self, X):\n",
        "      \"\"\"Return class label after unit step\"\"\"\n",
        "      values = np.dot(X, self.weights[1:]) + self.weights[0]\n",
        "      return np.where(values >= .5, 1, 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkwuU23o30Kn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "11f95ce0-225b-42db-a065-e8b99fd12bc0"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "pn = Perceptron(niter=50)\n",
        "bias = np.ones((X.shape[0], 1))\n",
        "data = np.append(bias, X, 1)\n",
        "target = train['Outcome'].values\n",
        "pn.fit(data, target.reshape((len(target), 1)))\n",
        "bias = np.ones((X_test.shape[0], 1))\n",
        "data = np.append(bias, X_test, 1)\n",
        "test_targets = test['Outcome'].values\n",
        "test_predictions = pn.predict(X_test)\n",
        "accuracy_score(test_predictions, test_targets)"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.609375"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6QR4oAW1xdyu"
      },
      "source": [
        "## Stretch Goals:\n",
        "\n",
        "- Research \"backpropagation\" to learn how weights get updated in neural networks (tomorrow's lecture). \n",
        "- Implement a multi-layer perceptron. (for non-linearly separable classes)\n",
        "- Try and implement your own backpropagation algorithm.\n",
        "- What are the pros and cons of the different activation functions? How should you decide between them for the different layers of a neural network?"
      ]
    }
  ]
}